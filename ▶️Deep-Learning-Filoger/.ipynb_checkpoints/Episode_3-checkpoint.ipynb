{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<section>\n",
    "    <div><img src=\"./images/Zahra-Amini.jpg\"></div>\n",
    "    <div style=\"display: flex\">\n",
    "        <div style=\"width:50%;background:#62060b\">\n",
    "            <div style=\"margin:auto;width:15rem\">\n",
    "                <img src=\"./images/filoger.png\" style=\"width:9rem; padding: 0.5rem 0;display:inline-block; vertical-align: middle\">\n",
    "                <p style=\"display:inline-block;font-family:monospace;font-weight:bold;font-size:15pt;color:white\">\n",
    "                Filoger\n",
    "                <p>\n",
    "            </div>\n",
    "        </div>\n",
    "        <div style=\"width:50%;background:#606368\">\n",
    "            <div style=\"margin:auto;width:23rem;margin-top: 3rem;\">\n",
    "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:#62060b;text-align:center\">\n",
    "                Deep Learning Course\n",
    "                <p>\n",
    "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:white;text-align:center\">\n",
    "                Episode 3\n",
    "                <p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chat-GPT Description about SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    <h3>تعریف کلی</h3>\n",
    "    گرادیان نزولی تصادفی (Stochastic Gradient Descent - SGD) یک الگوریتم بهینه‌سازی است که در یادگیری ماشین برای به‌روزرسانی پارامترهای مدل (مانند وزن‌ها در شبکه‌های عصبی) استفاده می‌شود. هدف این الگوریتم این است که تابع هزینه (یا خطا) را با تغییر تدریجی پارامترها به حداقل برساند.<br/>\n",
    "    <h3>گرادیان نزولی چیست؟</h3>\n",
    "    گرادیان نزولی (Gradient Descent) به‌طور کلی یک روش بهینه‌سازی است که برای پیدا کردن مقادیر بهینه پارامترهای یک مدل استفاده می‌شود. این الگوریتم پارامترها را در جهت مخالف گرادیان تابع هزینه حرکت می‌دهد تا به یک نقطه مینیمم (بهینه) برسد.\n",
    "    <h3>تفاوت‌های گرادیان نزولی تصادفی (SGD) با سایر روش‌ها</h3>\n",
    "    گرادیان نزولی می‌تواند به سه روش اصلی پیاده‌سازی شود:<br/>\n",
    "    <b>Batch Gradient Descent (گرادیان نزولی کامل):</b><br/>\n",
    "    در این روش، گرادیان‌ها با استفاده از کل مجموعه داده‌ها محاسبه می‌شوند. بنابراین، هر گام به‌روزرسانی پس از پردازش تمام داده‌ها انجام می‌شود.<br/>\n",
    "    این روش می‌تواند دقیق باشد اما برای مجموعه داده‌های بزرگ کند است و نیاز به حافظه زیادی دارد.<br/>\n",
    "    <b>Mini-Batch Gradient Descent (گرادیان نزولی دسته‌ای کوچک):</b><br/>\n",
    "    در این روش، داده‌ها به دسته‌های کوچک‌تر تقسیم می‌شوند و در هر گام، گرادیان‌ها با استفاده از یکی از این دسته‌ها محاسبه می‌شوند.<br/>\n",
    "    این روش تعادلی بین سرعت و دقت ایجاد می‌کند و به همین دلیل بسیار محبوب است.<br/>\n",
    "    <b>Stochastic Gradient Descent (گرادیان نزولی تصادفی):</b><br/>\n",
    "    در این روش، گرادیان‌ها برای هر نمونه به‌صورت جداگانه محاسبه می‌شوند. یعنی در هر گام، فقط یک نمونه از داده‌ها انتخاب شده و به‌روزرسانی انجام می‌شود.<br/>\n",
    "    این روش بسیار سریع‌تر از دو روش دیگر است زیرا برای هر به‌روزرسانی فقط یک نمونه را پردازش می‌کند.<br/>\n",
    "    <h3>نحوه کار SGD</h3>\n",
    "    <b>مقداردهی اولیه پارامترها:</b><br/>\n",
    "    ابتدا پارامترهای مدل به صورت تصادفی مقداردهی می‌شوند.<br/>\n",
    "    <b>انتخاب تصادفی یک نمونه:</b><br/>\n",
    "    در هر گام، یک نمونه به‌طور تصادفی از داده‌ها انتخاب می‌شود.<br/>\n",
    "    <b>محاسبه گرادیان:</b><br/>\n",
    "    گرادیان تابع هزینه نسبت به پارامترهای مدل بر اساس همین نمونه محاسبه می‌شود.<br/>\n",
    "    <b>به‌روزرسانی پارامترها:</b><br/>\n",
    "    پارامترها با استفاده از گرادیان محاسبه شده به‌روزرسانی می‌شون<br/>\n",
    "    (فرمول در جزوه هست)<br/>\n",
    "    <b>تکرار فرایند:</b><br/>\n",
    "    این فرآیند تا زمانی که مدل به همگرایی برسد یا تعداد مشخصی از تکرارها (epochs) انجام شود، ادامه می‌یابد.<br/>\n",
    "    <h3>مزایا و معایب SGD</h3>\n",
    "    <b>مزایا:</b><br/>\n",
    "    <b>سرعت بالا:</b> چون هر بار فقط یک نمونه پردازش می‌شود، الگوریتم بسیار سریع است.<br/>\n",
    "    <b>قابلیت یادگیری آنلاین:</b> مدل می‌تواند با داده‌های جدید در حین ورود آموزش ببیند.<br/>\n",
    "    <b>عبور از بهینه‌های محلی:</b> نوسانات کوچک در مسیر بهینه‌سازی می‌تواند مدل را از گیر افتادن در بهینه‌های محلی رها کند.<br/>\n",
    "    <b>معایب:</b><br/>\n",
    "    <b>نوسانات بالا:</b> به‌خاطر استفاده از یک نمونه در هر گام، نوسانات زیادی در مسیر بهینه‌سازی وجود دارد.<br/>\n",
    "    همگرایی کمتر دقیق: به دلیل نوسانات، ممکن است به نقطه مینیمم دقیق نرسد، بلکه در نزدیکی آن نوسان کند.<br/>\n",
    "    <h3>جمع‌بندی</h3>\n",
    "    گرادیان نزولی تصادفی (SGD) یک روش ساده و موثر برای بهینه‌سازی مدل‌ها در یادگیری ماشین است که به‌ویژه برای مجموعه داده‌های بزرگ و یادگیری آنلاین مناسب است. با این حال، به دلیل نوسانات بالا، ممکن است همگرایی به نقطه بهینه کمی دشوار باشد. اغلب برای مقابله با این مشکل از تکنیک‌های مختلفی مانند کاهش تدریجی نرخ یادگیری استفاده می‌شود."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Teacher Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of epochs and constant learning rate\n",
    "n_epochs = 50\n",
    "alpha = 0.1\n",
    "\n",
    "# initialize parameters randomly\n",
    "w = np.random.randn(2, 1)\n",
    "\n",
    "# Tranining loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):  # m should be defined as the number of samples in the dataset\n",
    "        random_index = np.random.randint(m)  # select a random index\n",
    "        xi = X_b[random_index : random_index + 1] # Extract features for selected sample\n",
    "        yi = y[random_index : random_index + 1] # Extract target for selected sample\n",
    "\n",
    "        # Computegradient of loss function\n",
    "        gradients = 2 * xi.T.dot(xi.dot(w) - yi)\n",
    "\n",
    "        # Update parameters\n",
    "        w = w- alpha * gradients # Apply gradient descent step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat-GPT Description about Teacher code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "\n",
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    <h3>معرفی مفاهیم</h3>\n",
    "    <b>Epochs (تعداد دوره ها):</b> تعداد دفعاتی که کل مجموعه داده برای آموزش مدل استفاده می شود.<br />\n",
    "    <b>Learning Rate (نرخ یادگیری):</b> یک مقدار عددی که مشخص می کند مدل چقدر سریع باید از خطاها یاد بگیرد.<br />\n",
    "    <b>Parameters (پارامترها):</b> وزن ها (weights) یا بایاس ها (biases) که در مدل یادگیری ماشین تنظیم می شوند.<br />\n",
    "    <b>Gradient Descent (نزول گرادیان):</b> یک الگوریتم برای بهینه سازی و به حداقل رساندن تابع  (loss (cost.<br />\n",
    "</section>\n",
    "<br />\n",
    "\n",
    "<code style=\"display:block\">#تعداد دوره ها و نرخ یادگیری را تنظیم کنید \n",
    "n_epochs = 50\n",
    "alpha = 0.1\n",
    "#پارامترها را به صورت تصادفی مقداردهی اولیه کنید   \n",
    "w = np.random.randn(2, 1)</code>\n",
    "\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    <b>n_epochs:</b> تعداد دوره هایی که مدل بر روی کل داده ها آموزش می بیند. در اینجا ۵۰ بار است.<br />\n",
    "    <b>alpha:</b> نرخ یادگیری که مشخص می کند پارامترها چقدر باید در هر مرحله تغییر کنند. در اینجا ۰.۱ است.<br />\n",
    "    <b>np.random.randn(2, 1):</b> این خط یک ماتریس ۲ در ۱ از اعداد تصادفی که از توزیع نرمال استاندارد (با میانگین صفر و انحراف معیار یک) تولید شده اند، ایجاد می کند. این ماتریس پارامترهای اولیه (وزن ها) را نشان می دهد.در یادگیری ماشین، ماتریس ها برای نمایش داده ها و پارامترهای مدل استفاده می شوند. در کد شما، ماتریس w پارامترهای مدل (وزن ها) را نشان می دهد. هر عنصر در این ماتریس نشان دهنده ی یک وزن است که مدل باید آن را یاد بگیرد تا بتواند پیش بینی های دقیقی انجام دهد. ماتریس 2 در 1 معمولاً در مدل های ساده تر مورد استفاده قرار می گیرد. به عنوان مثال، در رگرسیون خطی با یک ویژگی ورودی و یک بایاس، ماتریس وزن ها ممکن است 2 در 1 باشد، زیرا یک وزن برای ویژگی ورودی و یک وزن برای بایاس داریم.<br /><br />\n",
    "    <b>چرا مقداردهی اولیه وزن ها خارج از حلقه انجام می‌شود؟</b><br/>\n",
    "    حفظ تداوم و یادگیری:<br/>\n",
    "    هدف اصلی از اجرای الگوریتم Gradient Descent، به‌روزرسانی مداوم وزن‌ها به سمت مقادیر بهینه در طول زمان است. اگر هر بار در داخل حلقه for، بردار وزن‌ها مجدداً به‌صورت تصادفی مقداردهی اولیه شود، تمام تغییرات و به‌روزرسانی‌هایی که در تکرارهای قبلی انجام شده‌اند از بین خواهند رفت. بنابراین، وزن‌ها باید خارج از حلقه for مقداردهی شوند و سپس در هر تکرار، به‌روزرسانی‌های جدید روی همان وزن‌ها اعمال شود.\n",
    "    \n",
    "</section>\n",
    "<br />\n",
    "\n",
    "<code style=\"display:block\">#حلقه آموزشی\n",
    "for epoch in range(n_epochs): \n",
    "  m باید به عنوان تعداد نمونه ها (از قبل) در مجموعه داده تعریف شود \n",
    "   for i in range(m):\n",
    "         #یک ایندکس تصادفی را انتخاب کنید\n",
    "        random_index = np.random.randint(m)\n",
    "         #استخراج features برای نمونه انتخاب شده \n",
    "        xi = X_b[random_index : random_index + 1]         \n",
    "         #استخراج target برای نمونه انتخاب شده \n",
    "        yi = y[random_index : random_index + 1]  \n",
    "         #محاسبه گرادیان تابع هزینه\n",
    "        gradients = 2 * xi.T.dot(xi.dot(w) - yi) \n",
    "         #به روزرسانی پارامترها\n",
    "        w = w - alpha * gradients # اعمال گام نزول گرادیان\n",
    "</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    <b>for epoch in range(n_epochs):</b> یک حلقه برای تعداد دوره ها (n_epochs) ایجاد می کند. در هر دوره، مدل کل مجموعه داده را مرور می کند.<br />\n",
    "    <b>for i in range(m):</b> یک حلقه برای تعداد نمونه ها (m) در هر دوره ایجاد می کند. (در اینجا، m تعداد نمونه های داده است که باید در جای دیگری تعریف شده باشد.)<br />\n",
    "    <b>random_index = np.random.randint(m):</b> یک شاخص تصادفی بین ۰ و m (تعداد کل نمونه ها) انتخاب می کند. این به مدل کمک می کند تا هر بار یک نمونه تصادفی را برای آموزش انتخاب کند.<br />\n",
    "    <b>xi = X_b[random_index : random_index + 1]:</b> ویژگی های نمونه انتخاب شده را استخراج می کند. (فرض کنید X_b ماتریس ویژگی ها است.)<br />\n",
    "    <b>yi = y[random_index : random_index + 1]:</b> هدف یا خروجی واقعی نمونه انتخاب شده را استخراج می کند. (فرض کنید y بردار اهداف است.)<br /><br />\n",
    "    <h3>محاسبه گرادیان و به روزرسانی پارامترها</h3>\n",
    "    <b>gradients = 2 * xi.T.dot(xi.dot(w) - yi):</b> گرادیان تابع هزینه را محاسبه می کند.<br />\n",
    "    <b>xi.dot(w):</b> پیش بینی مدل برای نمونه انتخاب شده را محاسبه می کند.<br />\n",
    "    <b>xi.dot(w) - yi:</b> خطا (یا باقی مانده) بین پیش بینی مدل و هدف واقعی را محاسبه می کند.<br />\n",
    "    <b>xi.T.dot(...):</b> گرادیان را با توجه به وزن ها محاسبه می کند.<br />\n",
    "    <b>2 * ...:</b> ضرب در ۲ برای مقیاسبندی گرادیان (در اینجا تابع هزینه به احتمال زیاد مربع خطا است).<br />\n",
    "    <b>w = w - alpha * gradients:</b> پارامترها را به روزرسانی می کند.<br />\n",
    "    <b>w - alpha * gradients:</b> وزن ها را با کاهش مقدار گرادیان (با ضرب در نرخ یادگیری) به روزرسانی می کند. این عملیات باعث می شود که مدل به سمت کمینه تابع هزینه حرکت کند.<br />\n",
    "    <h3>جمع بندی</h3>\n",
    "    این کد یک مثال از آموزش یک مدل یادگیری ماشین با استفاده از الگوریتم نزول گرادیان است. به طور کلی، مراحل به شرح زیر هستند:<br />\n",
    "    تنظیم تعداد دوره ها و نرخ یادگیری.<br />\n",
    "    مقداردهی اولیه پارامترها به صورت تصادفی.<br />\n",
    "    برای هر دوره و هر نمونه:<br />\n",
    "    انتخاب یک نمونه تصادفی.<br />\n",
    "    محاسبه گرادیان تابع هزینه.<br />\n",
    "    به روزرسانی پارامترها با استفاده از نرخ یادگیری و گرادیان.<br />\n",
    "    امیدوارم که این توضیحات به شما در درک بهتر کد و مفاهیم اساسی یادگیری ماشین کمک کند. اگر سوال دیگری دارید یا نیاز به توضیح بیشتری دارید، حتماً بپرسید!<br />\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Teacher Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    <b>alpha:</b> لرنینگ ریت <br/>\n",
    "    <b>n_epochs:</b> تعداد مراحل آموزش مون هست<br/>\n",
    "    <b>w = np.random.randn(2, 1):</b> هارو بصورت رندوم مقدار دهی کرده با کمک نامپای<br/>\n",
    "    داخل لوپ داریم که به ازای هر epoch (تعداد مراحل آموزش) یه حلقه دیگه ای داریم.<br/>\n",
    "    m تعداد داده ها یا sample هامون هست<br/><br/>\n",
    "    تو لوپ داخلی هر بار اتفاق های زیر می افته:<br/>\n",
    "    ایندکس به صورت رندوم انتخاب میشه<br/>\n",
    "    مقدار x و y به صورت رندوم انتخاب میشود<br/>\n",
    "    گرادیان رو هر بار محاسبه میشه و بعد w آپدیت می شود<br/>\n",
    "</sction>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chat-GPT Example & Schematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"display:block\">   X1 ---- w1 ----|\r\n",
    "                  |      Y1\r\n",
    "   X2 ---- w2 ----|----->|\r\n",
    "                  |      |\r\n",
    "                  |      |  خروجی شبکه عصبی\r\n",
    "                  |      |\r\n",
    "                  |----->|\r\n",
    "                   </code>\n",
    " <section style=\"text-align:right;direction:rtl;\">\n",
    "    در این مثال ما 100 تا نمونه داریم و میخوایم 50 بار روی کل داده ها آموزش صورت بگیره.<br/>\n",
    "    ما 2 تا ورودی (x) و طبیعتا 2 تا وزن (w) خواهیم داشت. <br/>\n",
    "    ابتدا وزن هارو به صورت یک آرایه با 2 سطر و 1 ستون با کمک نامپای مقدار دهی اولیه میکنیم<br/>\n",
    "    نکته مهم اینجاست که چون اینجا دیتا ست نداریم، وزن ها، ویژگی ها و خروجی هارو بصورت رندوم خواهیم داشت<br/>\n",
    "    <b>X_b:</b> و ویژگی های ماست که بصورت آرایه ای با 2 ستون (چون 2 تا ویژگی داریم) و 100 سطر (به اندازه تعداد داده ها) خواهیم داشت<br/>\n",
    "    <b>y:</b> تارگت هامون که یه آرایه 1 سطری (چون به ازای ورودی ها فقط 1 خروجی داریم) و 100 سطر خواهد شد<br/>     |\r\n",
    "                 </code>     |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Weights:  [[ 1.06117154]\n",
      " [-0.03130794]]\n",
      "Optimized weights: [[-0.04667611]\n",
      " [ 0.50536946]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set number of epochs and constant learning rate\n",
    "n_epochs = 50\n",
    "alpha = 0.1\n",
    "\n",
    "# Initialize parameters randomly\n",
    "w = np.random.randn(2, 1)\n",
    "print(\"Old Weights: \", w)\n",
    "\n",
    "# Define X_b and y (Example data)\n",
    "# Suppose m = 100, n_features = 2\n",
    "m = 100\n",
    "X_b = np.random.randn(m, 2)\n",
    "y = np.random.randn(m, 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):  # Iterate over all samples\n",
    "        random_index = np.random.randint(m)  # Select a random index\n",
    "        xi = X_b[random_index : random_index + 1]  # Extract features for the selected sample\n",
    "        yi = y[random_index : random_index + 1]  # Extract target for the selected sample\n",
    "\n",
    "        # Compute gradient of loss function\n",
    "        gradients = 2 * xi.T.dot(xi.dot(w) - yi)\n",
    "\n",
    "        # Update parameters using the gradient descent step\n",
    "        w = w - alpha * gradients  # Apply gradient descent step\n",
    "\n",
    "print(\"Optimized weights:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 20 # size of mini-batch\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "w = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, batch_size):\n",
    "        xi = X_b_shuffled[i:i+batch_size]\n",
    "        yi = y_shuffled[i:i+batch_size]\n",
    "        gradients = 2/batch_size * xi.T.dot(xi.dot(w) - yi)\n",
    "        w = w - alpha * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat-GPT Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    این کد یک الگوریتم یادگیری ماشین است که با استفاده از گرادیان نزولی دسته‌ای کوچک (Mini-Batch Gradient Descent) برای بهینه‌سازی وزن‌ها (w) در یک مدل خطی استفاده می‌شود. به عبارت دیگر، این کد وزن‌های مدل را به گونه‌ای تنظیم می‌کند که خطای پیش‌بینی‌ها را کاهش دهد.<br/ ><br/ >\n",
    "    حالا بیایید این کد را خط به خط توضیح دهیم:<br/ >\n",
    "    <h3>پارامترها و مقادیر اولیه:</h3>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        n_epochs = 50\n",
    "        batch_size = 20 # اندازه دسته‌های کوچک\n",
    "        alpha = 0.1 # نرخ یادگیری\n",
    "    </code><br/ >\n",
    "    <b>n_epochs:</b> تعداد دوره‌ها (epochs) یا دفعاتی که کل مجموعه داده برای آموزش مدل استفاده می‌شود.<br/>\n",
    "    <b>batch_size:</b> اندازه دسته‌های کوچک (mini-batch) که در هر گام از الگوریتم گرادیان نزولی استفاده می‌شود.<br/>\n",
    "    <b>alpha:</b> نرخ یادگیری، که تعیین می‌کند چقدر وزن‌ها در هر گام به‌روز می‌شوند.<br/></b>\n",
    "    <h3>مقداردهی اولیه وزن‌ها:</h3>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        w = np.random.randn(2,1) # مقداردهی تصادفی اولیه\n",
    "    </code><br/ >\n",
    "    <b>w:</b> بردار وزن‌ها که به صورت تصادفی مقداردهی اولیه شده است. فرض می‌شود که مدل خطی شما دارای ۲ ویژگی ورودی است (به همین دلیل، ابعاد آن (2,1) است).<br/ >\n",
    "    <h3>حلقه اصلی (epoch):</h3>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        for epoch in range(n_epochs):\n",
    "    </code><br/ >\n",
    "    این حلقه به تعداد n_epochs (در اینجا ۵۰) تکرار می‌شود، یعنی مدل ۵۰ بار بر روی کل داده‌ها آموزش داده می‌شود.<br/ >\n",
    "    <h3>مخلوط‌کردن داده‌ها:</h3>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_b_shuffled = X_b[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "    </code><br/>\n",
    "    به قطعه کد زیر نگاه کنین. خروجی آن بصورت array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6]) # random خواهد بود\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">np.random.permutation(10)</code><br/> \n",
    "    <b>shuffled_indices:</b> ایندکس‌های داده‌ها را به صورت تصادفی مخلوط می‌کند.<br/>\n",
    "   <b>X_b_shuffled و y_shuffled:</b> داده های ورودی و تارگت به هم خورده هستند.<br/>\n",
    "    اگر m ما 100 باشد، لیستی که از 0 تا 99 بصورت نامنظم هست داخل براکت به ورودی ها و تارگت های اصلی میدهیم تا ترتیبشان را بهم بزنیم و در نهایت X_b_shuffled و y_shuffled را داشته باشیم<br/>\n",
    "    دقت کنین که باید ترتیب x و y ها دقیقا مشابه باشند تا کار خراب نشود. به عبارتی مهم است که خروجی ایندکس مثلا 16 در لیست y_shuffled دقیقا برابر با ورودی های ایندکس 16 X_b_shuffled باشد<br/>\n",
    "    در اینجا همین اتفاق خواهد افتاد زیرا X و Y را داریم با لیست shuffled_indices که در هر epoch ثابت است بهم میزنیم..\n",
    "    <h3>حلقه دسته‌های کوچک (mini-batch):</h3>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_b_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "    </code><br/>    \n",
    "    این حلقه داده‌های ورودی و برچسب‌ها را به دسته‌های کوچک تقسیم می‌کند و به صورت دسته‌ای پردازش می‌کند:<br/>\n",
    "    <b>xi:</b> دسته‌ای از داده‌های ورودی.<br/>\n",
    "    <b>yi:</b> دسته‌ای از برچسب‌ها (خروجی‌های هدف).<br/>\n",
    "    <h3>محاسبه گرادیان و به‌روزرسانی وزن‌ها:</h3>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        gradients = 2/batch_size * xi.T.dot(xi.dot(w) - yi)\n",
    "        w = w - alpha * gradients\n",
    "    </code><br/>\n",
    "    <b>gradients:</b> گرادیان (شیب) تابع هزینه نسبت به وزن‌ها محاسبه می‌شود.<br/>\n",
    "    <b>xi.dot(w):</b> محاسبه پیش‌بینی مدل برای داده‌های xi.<br/>\n",
    "    <b>xi.dot(w) - yi:</b> خطای پیش‌بینی.<br/>\n",
    "    <b>xi.T.dot(...):</b> ضرب ماتریس ترانهاده xi در خطا.<br/>\n",
    "    <b>2/batch_size:</b> میانگین‌گیری گرادیان‌ها.<br/>\n",
    "    <b>w = w - alpha * gradients:</b> وزن‌ها با استفاده از گرادیان و نرخ یادگیری به‌روزرسانی می‌شوند.<br/>\n",
    "    <h3>پایان دوره‌ها (epochs):</h3>\n",
    "    پس از ۵۰ دوره، وزن‌های مدل آموزش داده می‌شوند و آماده استفاده برای پیش‌بینی هستند.\n",
    "    <h3>خلاصه:</h3>\n",
    "    این کد یک مدل خطی را با استفاده از گرادیان نزولی دسته‌ای کوچک (Mini-Batch Gradient Descent) آموزش می‌دهد. در هر دوره، داده‌ها مخلوط شده و به دسته‌های کوچک تقسیم می‌شوند، سپس گرادیان محاسبه شده و وزن‌ها به‌روزرسانی می‌شوند. نرخ یادگیری (alpha) تعیین می‌کند که وزن‌ها با چه سرعتی تنظیم شوند.\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    یدونه سایز بچ اضافه شده که گفتیم 20 تا باشه.<br/>\n",
    "    w رو مقداردهی اولیه کردیم .<br/>\n",
    "    اینجا بازهم 2 تا حلقه تو در تو داریم ولی با این تفاوت که در هر epoch ترتیب X و Y هایمان فرق خواهد کرد.<br/>\n",
    "    در هر epoch یک حلقه ای داریم که متغییر آن i از صفر شروع شده و تا m می رود و در هر گام به اندازه batch_size به آن اضافه می شود.<br/>\n",
    "    مثلا اگه batch size مون 8 باشه و m برابر 24 باشه لیستی که توش حلقه میزنه به این صورت خواهد بود:[0,8,16,24].<br/>\n",
    "    xi و yi رو برداشته و گرادیان رو محاسبه کرده و سپس w آپدیت کرده.\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat-GPT Example & Schematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"display:block\">   X1 ---- w1 ----|\r\n",
    "                  |      Y1\r\n",
    "   X2 ---- w2 ----|----->|\r\n",
    "                  |      |\r\n",
    "                  |      |  خروجی شبکه عصبی\r\n",
    "                  |      |\r\n",
    "                  |----->|\r\n",
    "                    </code>\n",
    "\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    <h3>توضیح شماتیک:</h3>\n",
    "    X1 و X2 ورودی‌های شبکه عصبی هستند.<br/>\n",
    "    w1 و w2 پارامترهای وزن بین ورودی‌ها و خروجی هستند.<br/>\n",
    "    Y1 خروجی شبکه عصبی است که به صورت خطی از ورودی‌ها و وزن‌ها محاسبه می‌شود.<br/>\n",
    "    در هر مرحله از به‌روزرسانی وزن‌ها، به جای یک نمونه، یک mini-batch (دسته کوچک) از داده‌ها برای محاسبه گرادیان استفاده می‌شود.<br/>\n",
    "    batch_size اندازه mini-batch را تعیین می‌کند. در این مثال، هر mini-batch شامل 20 نمونه است.<br/>\n",
    "    در هر epoch، داده‌ها به‌صورت تصادفی مرتب شده و سپس به mini-batch‌های کوچکتر تقسیم می‌شوند.<br/>\n",
    "    برای هر mini-batch، گرادیان‌ها محاسبه شده و پارامتر وزن‌ها (w) به‌روزرسانی می‌شوند.<br/>\n",
    "    این فرآیند بهینه‌سازی تکرار می‌شود تا زمانی که تعداد epochها کامل شود.<br/>\n",
    "    \n",
    "</section>     |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights: [[-0.1060252 ]\n",
      " [ 0.03746117]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set number of epochs, batch size, and learning rate\n",
    "n_epochs = 50\n",
    "batch_size = 20  # Size of mini-batch\n",
    "alpha = 0.1  # Learning rate\n",
    "\n",
    "# Initialize parameters randomly\n",
    "w = np.random.randn(2, 1)\n",
    "\n",
    "# Define X_b and y (Example data)\n",
    "# Suppose m = 100, n_features = 2\n",
    "m = 100\n",
    "X_b = np.random.randn(m, 2)\n",
    "y = np.random.randn(m, 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(m)  # Shuffle indices\n",
    "    X_b_shuffled = X_b[shuffled_indices]  # Shuffle features\n",
    "    y_shuffled = y[shuffled_indices]  # Shuffle target values\n",
    "    for i in range(0, m, batch_size):\n",
    "        xi = X_b_shuffled[i:i+batch_size]  # Extract mini-batch features\n",
    "        yi = y_shuffled[i:i+batch_size]  # Extract mini-batch targets\n",
    "        gradients = 2/batch_size * xi.T.dot(xi.dot(w) - yi)  # Compute gradients\n",
    "        w = w - alpha * gradients  # Update parameters using the gradient descent step\n",
    "\n",
    "print(\"Optimized weights:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Defrence Between SGD & MGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "     انتخاب روش‌های مختلف برای نمونه‌گیری و ترتیب‌گذاری داده‌ها در Stochastic Gradient Descent (SGD) و Mini-Batch Gradient Descent (MGD) به دلایل خاصی انجام می‌شود که به نحوه‌ی کار هر کدام از این الگوریتم‌ها مربوط می‌شود. اجازه دهید تفاوت‌ها را بررسی کنیم.<br/>\n",
    "    <h3>Stochastic Gradient Descent (SGD) و استفاده از np.random.randint(m)</h3>\n",
    "    در Stochastic Gradient Descent (SGD)، هدف این است که در هر گام به‌روزرسانی، تنها یک نمونه از مجموعه داده‌ها انتخاب و پردازش شود. به همین دلیل:<br/>\n",
    "    از تابع np.random.randint(m) استفاده می‌شود تا یک ایندکس تصادفی از کل مجموعه داده‌ها انتخاب شود.<br/>\n",
    "    با این روش، در هر تکرار (iteration) تنها یک نمونه داده تصادفی از مجموعه داده‌ها گرفته می‌شود. این انتخاب کاملاً مستقل است و ممکن است یک نمونه چندین بار انتخاب شود و برخی از نمونه‌ها اصلاً انتخاب نشوند.<br/>\n",
    "    <h3>Mini-Batch Gradient Descent (MGD) و استفاده از np.random.permutation(m)</h3>\n",
    "    در Mini-Batch Gradient Descent، هدف این است که در هر گام به‌روزرسانی، یک دسته (batch) از نمونه‌ها به‌طور همزمان پردازش شود. به همین دلیل:<br/>\n",
    "    از np.random.permutation(m) استفاده می‌شود تا تمام ایندکس‌های داده‌ها مخلوط شوند و یک ترتیب تصادفی جدید ایجاد شود.<br/>\n",
    "    سپس داده‌ها به دسته‌های کوچک تقسیم می‌شوند. این مخلوط کردن تضمین می‌کند که هر نمونه فقط یک بار در هر epoch استفاده می‌شود و ترتیب نمونه‌ها تغییر می‌کند تا مدل دچار الگوهای غیرمطلوب نشود.<br/>\n",
    "    بنابراین، در هر تکرار، یک mini-batch از داده‌های مخلوط‌شده پردازش می‌شود.\n",
    "    <h3>چرا این تفاوت مهم است؟</h3>\n",
    "    <b>نحوه‌ی انتخاب نمونه‌ها:</b>\n",
    "    در SGD، هر تکرار فقط روی یک نمونه داده اجرا می‌شود و نمونه به صورت تصادفی از کل داده‌ها انتخاب می‌شود. انتخاب نمونه‌های تصادفی مستقل است و ممکن است تکراری باشد.<br/>\n",
    "    در MGD، یک mini-batch از داده‌ها پردازش می‌شود. ابتدا کل داده‌ها مخلوط می‌شوند تا هر دسته به صورت تصادفی انتخاب شود و از تمام داده‌ها استفاده شود.<br/>\n",
    "    <b>تضمین استفاده از کل داده‌ها:</b><br/>\n",
    "    در MGD، از np.random.permutation(m) استفاده می‌شود تا مطمئن شویم که هر داده در طول یک epoch حداقل یک بار انتخاب می‌شود.<br/>\n",
    "    در مقابل، در SGD با np.random.randint(m) ممکن است بعضی نمونه‌ها در یک epoch انتخاب نشوند یا چندین بار انتخاب شوند.\n",
    "    <h3>کاربرد و رفتار الگوریتم:</h3>\n",
    "    <b>SGD:</b> بیشتر برای یادگیری سریع و آنلاین استفاده می‌شود، زیرا فقط یک نمونه پردازش می‌شود و هر گام به‌روزرسانی بسیار سریع انجام می‌شود.<br/>\n",
    "    <b>MGD:</b> برای یادگیری با سرعت قابل قبول و همگرایی پایدارتر استفاده می‌شود، زیرا از یک دسته داده برای هر گام استفاده می‌کند.\n",
    "    <h3>جمع‌بندی</h3>\n",
    "    np.random.randint(m) در SGD به ما یک نمونه تصادفی و مستقل از داده‌ها می‌دهد.<br/>\n",
    "    np.random.permutation(m) در MGD برای مخلوط کردن کل داده‌ها و تقسیم آنها به دسته‌های کوچک استفاده می‌شود.<br/>\n",
    "    این دو روش انتخاب تصادفی با توجه به ماهیت و اهداف هر الگوریتم انتخاب می‌شوند تا به نتایج بهینه‌تر و موثرتر دست یابند.<br/>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_epochs = 50\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "w = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(w) - y)\n",
    "    w = w - alpha * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    <h3>شرح کلی کد</h3>\n",
    "    این کد یک پیاده‌سازی ساده از الگوریتم Batch Gradient Descent است که برای بهینه‌سازی پارامترهای یک مدل خطی استفاده می‌شود. هدف این الگوریتم به‌روزرسانی پارامترهای مدل (در اینجا وزن‌ها) به گونه‌ای است که تابع هزینه (که نشان‌دهنده‌ی خطای مدل است) به حداقل برسد.\n",
    "    <h3>شرح خط به خط کد</h3>\n",
    "    <b>تعریف تعداد epochs و نرخ یادگیری:</b>\n",
    "</section><br/>\n",
    "\n",
    "<code style=\"display:block; text-align:left;direction:ltr\">n_epochs = 50\n",
    "alpha = 0.1 # learning rate\n",
    "</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    <b>n_epochs = 50:</b> تعداد epoch‌ها را تعیین می‌کند. هر epoch به معنای یک بار مرور کامل بر روی کل داده‌ها است.<br/>\n",
    "    <b>alpha = 0.1:</b>نرخ یادگیری است. این مقدار تعیین می‌کند که مدل در هر گام به چه میزان وزن‌ها را تغییر دهد. نرخ یادگیری کوچک باعث همگرایی آهسته‌تر اما دقیق‌تر می‌شود، در حالی که نرخ یادگیری بزرگ‌تر ممکن است باعث نوسان یا عدم همگرایی شود.<br/>\n",
    "    <h4>مقداردهی اولیه وزن‌ها:</h4>\n",
    "</section><br/>\n",
    "\n",
    "<code style=\"display:block; text-align:left;direction:ltr\">w = np.random.randn(2,1)</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    <b>w:</b> بردار وزن‌ها است که به صورت تصادفی مقداردهی اولیه می‌شود. این بردار ابعاد (2,1) دارد که نشان می‌دهد مدل دو ویژگی (متغیر مستقل) دارد.\n",
    "    <b>np.random.randn(2,1):</b> یک بردار تصادفی از توزیع نرمال استاندارد با ابعاد ۲×۱ تولید می‌کند. مقداردهی تصادفی به وزن‌ها کمک می‌کند تا الگوریتم از یک نقطه شروع اولیه به سمت بهینه حرکت کند.\n",
    "    <h4>شروع حلقه اصلی برای به‌روزرسانی وزن‌ها (حلقه epochs):</h4>\n",
    "</section><br/>\n",
    "\n",
    "<code style=\"display:block; text-align:left;direction:ltr\">for epoch in range(n_epochs):</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    این حلقه برای تعداد مشخصی از epoch‌ها (n_epochs) اجرا می‌شود. در هر epoch، وزن‌ها به‌روزرسانی می‌شوند تا مدل بهینه‌تر شود.\n",
    "    <h4>محاسبه گرادیان‌ها:</h4>\n",
    "</section><br/>\n",
    "\n",
    "<code style=\"display:block; text-align:left;direction:ltr\">gradients = 2/m * X_b.T.dot(X_b.dot(w) - y)</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    <b>X_b.dot(w):</b> ضرب ماتریس ویژگی‌ها (X_b) در بردار وزن‌ها (w). این مقدار پیش‌بینی مدل برای خروجی‌ها است.<br/>\n",
    "    <b>X_b.dot(w) - y:</b> اختلاف بین پیش‌بینی مدل و مقادیر واقعی (y) را محاسبه می‌کند. این مقدار خطای مدل است.<br/>\n",
    "    <b>X_b.T.dot(X_b.dot(w) - y):</b> گرادیان یا شیب تابع هزینه نسبت به وزن‌ها را محاسبه می‌کند. این محاسبه نشان می‌دهد که چگونه باید وزن‌ها تغییر کنند تا خطا کاهش یابد.<br/>\n",
    "    <b>2/m:</b> میانگین‌گیری و تعدیل برای کاهش حساسیت به اندازه داده‌ها (اینجا m تعداد نمونه‌های داده است).\n",
    "    <h4>به‌روزرسانی وزن‌ها:</h4>\n",
    "</section><br/>\n",
    "\n",
    "<code style=\"display:block; text-align:left;direction:ltr\">w = w - alpha * gradients</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    وزن‌ها با استفاده از نرخ یادگیری (alpha) و گرادیان محاسبه شده، به‌روزرسانی می‌شوند.<br/>\n",
    "    <b>w - alpha * gradients:</b> این معادله نشان می‌دهد که وزن‌ها در جهت مخالف گرادیان (شیب) حرکت می‌کنند تا خطای مدل کاهش یابد. به عبارت دیگر، وزن‌ها به‌سمت کمینه تابع هزینه جابه‌جا می‌شوند.\n",
    "    <h3>فرآیند کلی کد</h3>\n",
    "    ابتدا وزن‌ها به صورت تصادفی مقداردهی اولیه می‌شوند.<br/>\n",
    "    سپس در هر epoch، تمام داده‌ها مرور شده و بر اساس آن‌ها گرادیان (میزان تغییر مورد نیاز در وزن‌ها) محاسبه می‌شود.<br/>\n",
    "    وزن‌ها به‌روزرسانی می‌شوند تا خطای مدل کاهش یابد.<br/>\n",
    "    این فرآیند تکرار می‌شود تا مدل به سمت یک جواب بهینه همگرا شود.\n",
    "    <h3>نتیجه نهایی</h3>\n",
    "    این کد الگوریتم Batch Gradient Descent را پیاده‌سازی می‌کند که در آن کل مجموعه داده‌ها در هر تکرار پردازش می‌شود. وزن‌ها به‌طور مداوم در طول 50 epoch به‌روزرسانی می‌شوند تا مدل به بهترین نتیجه ممکن برای پیش‌بینی برسد.\n",
    "</section>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    همون کار هایی که برای اون 2 نوع دیگه داشتیم انجام میدیم. ولی با این تفاوت که همه دیتا هارو با یک حلقه پوشش داده شده و هر بار که حلقه تکرار می شود، وزن ها آپدیت می شود.\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat-GPT Example & Schematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"display:block; text-align:left;direction:ltr\">   X1 ---- w1 ----|\r\n",
    "                  |      Y1\r\n",
    "   X2 ---- w2 ----|----->|\r\n",
    "                  |      |\r\n",
    "                  |      |  خروجی شبکه عصبی\r\n",
    "                  |      |\r\n",
    "                  |----->|\r\n",
    "                    </code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl\">\n",
    "    <h3>توضیح کد:</h3>\n",
    "    gradients گرادیان تابع هزینه است که با استفاده از کل مجموعه داده‌ها محاسبه می‌شود.<br/>\n",
    "    در هر epoch، وزن‌ها (w) تنها یک بار با استفاده از گرادیان محاسبه‌شده به‌روزرسانی می‌شوند.<br/>\n",
    "    این روش به صورت دسته‌ای (Batch) عمل می‌کند و در هر تکرار از کل داده‌ها برای محاسبه گرادیان استفاده می‌کند، که این کار دقت بیشتری نسبت به Stochastic و Mini-batch Gradient Descent دارد.\n",
    "    <h3>مزایا و معایب:</h3>\n",
    "    <b>مزیت:</b> محاسبه دقیق‌تر گرادیان و به‌روزرسانی‌های منظم‌تر که می‌تواند منجر به همگرایی بهتر شود.<br/>\n",
    "    <b>عیب:</b> نیاز به محاسبات سنگین‌تر و حافظه بیشتر برای پردازش کل داده‌ها در هر تکرار.\n",
    "</section>     |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights: [[-0.14474351]\n",
      " [ 0.07988114]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set number of epochs and learning rate\n",
    "n_epochs = 50\n",
    "alpha = 0.1  # Learning rate\n",
    "\n",
    "# Initialize parameters randomly\n",
    "w = np.random.randn(2, 1)\n",
    "\n",
    "# Define X_b and y (Example data)\n",
    "# Suppose m = 100, n_features = 2\n",
    "m = 100\n",
    "X_b = np.random.randn(m, 2)\n",
    "y = np.random.randn(m, 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(w) - y)  # Compute gradients using all samples\n",
    "    w = w - alpha * gradients  # Update parameters using the gradient descent step\n",
    "\n",
    "print(\"Optimized weights:\", w)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
