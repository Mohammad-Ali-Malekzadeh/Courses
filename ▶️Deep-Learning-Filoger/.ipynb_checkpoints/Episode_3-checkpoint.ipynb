{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "    <div><img src=\"./images/Zahra-Amini.jpg\"></div>\n",
    "    <div style=\"display:flex\">\n",
    "        <div style=\"width:50%;background:#62060b\">\n",
    "            <div style=\"width:max-content;margin:auto;width:20rem\">\n",
    "                <img src=\"./images/filoger.png\" style=\"width:11rem; padding: 0.5rem 0;display:inline-block\">\n",
    "                <p style=\"display:inline-block;font-family:monospace;font-weight:bold;font-size:15pt;color:white\">\n",
    "                Filoger\n",
    "                <p>\n",
    "            </div>\n",
    "        </div>\n",
    "        <div style=\"width:50%;background:#606368\">\n",
    "            <div style=\"margin:auto;width:23rem;margin-top: 3rem;\">\n",
    "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:#62060b\">\n",
    "                Deep Learning Course\n",
    "                <p>\n",
    "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:white;text-align:center\">\n",
    "                Episode 3\n",
    "                <p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of epochs and constant learning rate\n",
    "n_epochs = 50\n",
    "alpha = 0.1\n",
    "\n",
    "# initialize parameters randomly\n",
    "w = np.random.randn(2, 1)\n",
    "\n",
    "# Tranining loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):  # m should be defined as the number of samples in the dataset\n",
    "        random_index = np.random.randint(m)  # select a random index\n",
    "        xi = X_b[random_index : random_index + 1] # Extract features for selected sample\n",
    "        yi = y[random_index : random_index + 1] # Extract target for selected sample\n",
    "\n",
    "        # Computegradient of loss function\n",
    "        gradients = 2 * xi.T.dot(xi.dot(w) - yi)\n",
    "\n",
    "        # Update parameters\n",
    "        w = w- alpha * gradients # Apply gradient descent step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat-GPT Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "# معرفی مفاهیم\n",
    "\n",
    "Epochs (تعداد دوره ها): تعداد دفعاتی که کل مجموعه داده برای آموزش مدل استفاده می شود.\n",
    "\n",
    "Learning Rate (نرخ یادگیری): یک مقدار عددی که مشخص می کند مدل چقدر سریع باید از خطاها یاد بگیرد.\n",
    "\n",
    "Parameters (پارامترها): وزن ها (weights) یا بایاس ها (biases) که در مدل یادگیری ماشین تنظیم می شوند.\n",
    "\n",
    "Gradient Descent (نزول گرادیان): یک الگوریتم برای بهینه سازی و به حداقل رساندن تابع هزینه.\n",
    "\n",
    "# کد با توضیحات\n",
    "\n",
    "\n",
    "<code style=\"width=100%\">\n",
    "    #تعداد دوره ها و نرخ یادگیری را تنظیم کنید\n",
    "    \n",
    "    n_epochs = 50\n",
    "    alpha = 0.1\n",
    "\n",
    "    #پارامترها را به صورت تصادفی مقداردهی اولیه کنید\n",
    "    \n",
    "    w = np.random.randn(2, 1)\n",
    "</code>\n",
    "\n",
    "n_epochs: تعداد دوره‌هایی که مدل بر روی کل داده‌ها آموزش می‌بیند. در اینجا ۵۰ بار است.\n",
    "\n",
    "alpha: نرخ یادگیری که مشخص می‌کند پارامترها چقدر باید در هر مرحله تغییر کنند. در اینجا ۰.۱ است.\n",
    "np.random.randn(2, 1): این خط یک ماتریس ۲ در ۱ از اعداد تصادفی که از توزیع نرمال استاندارد (با میانگین صفر و انحراف معیار یک) تولید شده‌اند، ایجاد می‌کند. این ماتریس پارامترهای اولیه (وزن‌ها) را نشان می‌دهد.\n",
    "python\n",
    "Copy code\n",
    "# حلقه آموزشی\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):  # m باید به عنوان تعداد نمونه‌ها در مجموعه داده تعریف شود\n",
    "        random_index = np.random.randint(m)  # یک شاخص تصادفی را انتخاب کنید\n",
    "        xi = X_b[random_index : random_index + 1] # استخراج ویژگی‌ها برای نمونه انتخاب شده\n",
    "        yi = y[random_index : random_index + 1] # استخراج هدف برای نمونه انتخاب شده\n",
    "\n",
    "        # محاسبه گرادیان تابع هزینه\n",
    "        gradients = 2 * xi.T.dot(xi.dot(w) - yi)\n",
    "\n",
    "        # به‌روزرسانی پارامترها\n",
    "        w = w - alpha * gradients # اعمال گام نزول گرادیان\n",
    "for epoch in range(n_epochs): یک حلقه برای تعداد دوره‌ها (n_epochs) ایجاد می‌کند. در هر دوره، مدل کل مجموعه داده را مرور می‌کند.\n",
    "for i in range(m): یک حلقه برای تعداد نمونه‌ها (m) در هر دوره ایجاد می‌کند. (در اینجا، m تعداد نمونه‌های داده است که باید در جای دیگری تعریف شده باشد.)\n",
    "random_index = np.random.randint(m): یک شاخص تصادفی بین ۰ و m (تعداد کل نمونه‌ها) انتخاب می‌کند. این به مدل کمک می‌کند تا هر بار یک نمونه تصادفی را برای آموزش انتخاب کند.\n",
    "xi = X_b[random_index : random_index + 1]: ویژگی‌های نمونه انتخاب شده را استخراج می‌کند. (فرض کنید X_b ماتریس ویژگی‌ها است.)\n",
    "yi = y[random_index : random_index + 1]: هدف یا خروجی واقعی نمونه انتخاب شده را استخراج می‌کند. (فرض کنید y بردار اهداف است.)\n",
    "محاسبه گرادیان و به‌روزرسانی پارامترها\n",
    "gradients = 2 * xi.T.dot(xi.dot(w) - yi): گرادیان تابع هزینه را محاسبه می‌کند.\n",
    "\n",
    "xi.dot(w): پیش‌بینی مدل برای نمونه انتخاب شده را محاسبه می‌کند.\n",
    "xi.dot(w) - yi: خطا (یا باقی‌مانده) بین پیش‌بینی مدل و هدف واقعی را محاسبه می‌کند.\n",
    "xi.T.dot(...): گرادیان را با توجه به وزن‌ها محاسبه می‌کند.\n",
    "2 * ...: ضرب در ۲ برای مقیاس‌بندی گرادیان (در اینجا تابع هزینه به احتمال زیاد مربع خطا است).\n",
    "w = w - alpha * gradients: پارامترها را به‌روزرسانی می‌کند.\n",
    "\n",
    "w - alpha * gradients: وزن‌ها را با کاهش مقدار گرادیان (با ضرب در نرخ یادگیری) به‌روزرسانی می‌کند. این عملیات باعث می‌شود که مدل به سمت کمینه تابع هزینه حرکت کند.\n",
    "جمع‌بندی\n",
    "این کد یک مثال از آموزش یک مدل یادگیری ماشین با استفاده از الگوریتم نزول گرادیان است. به طور کلی، مراحل به شرح زیر هستند:\n",
    "\n",
    "تنظیم تعداد دوره‌ها و نرخ یادگیری.\n",
    "مقداردهی اولیه پارامترها به صورت تصادفی.\n",
    "برای هر دوره و هر نمونه:\n",
    "انتخاب یک نمونه تصادفی.\n",
    "محاسبه گرادیان تابع هزینه.\n",
    "به‌روزرسانی پارامترها با استفاده از نرخ یادگیری و گرادیان.\n",
    "امیدوارم که این توضیحات به شما در درک بهتر کد و مفاهیم اساسی یادگیری ماشین کمک کند. اگر سوال دیگری دارید یا نیاز به توضیح بیشتری دارید، حتماً بپرسید!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "alpha = 0.1  # learning rate\n",
    "\n",
    "w = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(w) - y)\n",
    "    w = w - alpha * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 20 # size of mini-batch\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "w = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, batch_size):\n",
    "        xi = X_b_shuffled[i:i+batch_size]\n",
    "        yi = y_shuffled[i:i+batch_size]\n",
    "        gradients = 2/batch_size * xi.T.dot(xi.dot(w) - yi)\n",
    "        w = w - alpha * gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
