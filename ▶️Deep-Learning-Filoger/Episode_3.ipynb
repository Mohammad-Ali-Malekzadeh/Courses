{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<section>\n",
       "    <div><img src=\"./images/Zahra-Amini.jpg\"></div>\n",
       "    <div style=\"display: flex\">\n",
       "        <div style=\"width:50%;background:#62060b\">\n",
       "            <div style=\"width:max-content;margin:auto;\">\n",
       "                <img src=\"./images/filoger.png\" style=\"width:9rem; padding: 0.5rem 0;display:inline-block; vertical-align: middle\">\n",
       "                <p style=\"display:inline-block;font-family:monospace;font-weight:bold;font-size:15pt;color:white\">\n",
       "                Filoger\n",
       "                <p>\n",
       "            </div>\n",
       "        </div>\n",
       "        <div style=\"width:50%;background:#606368\">\n",
       "            <div style=\"margin:auto;width:23rem;margin-top: 3rem;\">\n",
       "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:#62060b;text-align:center\">\n",
       "                Deep Learning Course\n",
       "                <p>\n",
       "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:white;text-align:center\">\n",
       "                Episode 3\n",
       "                <p>\n",
       "            </div>\n",
       "        </div>\n",
       "    </div>\n",
       "</section>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<section>\n",
    "    <div><img src=\"./images/Zahra-Amini.jpg\"></div>\n",
    "    <div style=\"display: flex\">\n",
    "        <div style=\"width:50%;background:#62060b\">\n",
    "            <div style=\"width:max-content;margin:auto;\">\n",
    "                <img src=\"./images/filoger.png\" style=\"width:9rem; padding: 0.5rem 0;display:inline-block; vertical-align: middle\">\n",
    "                <p style=\"display:inline-block;font-family:monospace;font-weight:bold;font-size:15pt;color:white\">\n",
    "                Filoger\n",
    "                <p>\n",
    "            </div>\n",
    "        </div>\n",
    "        <div style=\"width:50%;background:#606368\">\n",
    "            <div style=\"margin:auto;width:23rem;margin-top: 3rem;\">\n",
    "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:#62060b;text-align:center\">\n",
    "                Deep Learning Course\n",
    "                <p>\n",
    "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:white;text-align:center\">\n",
    "                Episode 3\n",
    "                <p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of epochs and constant learning rate\n",
    "n_epochs = 50\n",
    "alpha = 0.1\n",
    "\n",
    "# initialize parameters randomly\n",
    "w = np.random.randn(2, 1)\n",
    "\n",
    "# Tranining loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):  # m should be defined as the number of samples in the dataset\n",
    "        random_index = np.random.randint(m)  # select a random index\n",
    "        xi = X_b[random_index : random_index + 1] # Extract features for selected sample\n",
    "        yi = y[random_index : random_index + 1] # Extract target for selected sample\n",
    "\n",
    "        # Computegradient of loss function\n",
    "        gradients = 2 * xi.T.dot(xi.dot(w) - yi)\n",
    "\n",
    "        # Update parameters\n",
    "        w = w- alpha * gradients # Apply gradient descent step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat-GPT Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "معرفی مفاهیم\n",
    "\n",
    "<b>Epochs (تعداد دوره ها):</b> تعداد دفعاتی که کل مجموعه داده برای آموزش مدل استفاده می شود.\n",
    "\n",
    "<b>Learning Rate (نرخ یادگیری):</b> یک مقدار عددی که مشخص می کند مدل چقدر سریع باید از خطاها یاد بگیرد.\n",
    "\n",
    "<b>Parameters (پارامترها):</b> وزن ها (weights) یا بایاس ها (biases) که در مدل یادگیری ماشین تنظیم می شوند.\n",
    "\n",
    "<b>Gradient Descent (نزول گرادیان):</b> یک الگوریتم برای بهینه سازی و به حداقل رساندن تابع  (loss (cost.\n",
    "\n",
    "کد با توضیحات\n",
    "\n",
    "\n",
    "<code style=\"display:block\">#تعداد دوره ها و نرخ یادگیری را تنظیم کنید \n",
    "n_epochs = 50\n",
    "alpha = 0.1\n",
    "پارامترها را به صورت تصادفی مقداردهی اولیه کنید   \n",
    "w = np.random.randn(2, 1)\n",
    "</code>\n",
    "\n",
    "<b>n_epochs:</b> تعداد دوره هایی که مدل بر روی کل داده ها آموزش می بیند. در اینجا ۵۰ بار است.\n",
    "\n",
    "<b>alpha:</b> نرخ یادگیری که مشخص می کند پارامترها چقدر باید در هر مرحله تغییر کنند. در اینجا ۰.۱ است.\n",
    "\n",
    "<b>np.random.randn(2, 1):</b> این خط یک ماتریس ۲ در ۱ از اعداد تصادفی که از توزیع نرمال استاندارد (با میانگین صفر و انحراف معیار یک) تولید شده اند، ایجاد می کند. این ماتریس پارامترهای اولیه (وزن ها) را نشان می دهد.در یادگیری ماشین، ماتریس ها برای نمایش داده ها و پارامترهای مدل استفاده می شوند. در کد شما، ماتریس w پارامترهای مدل (وزن ها) را نشان می دهد. هر عنصر در این ماتریس نشان دهنده ی یک وزن است که مدل باید آن را یاد بگیرد تا بتواند پیش بینی های دقیقی انجام دهد. ماتریس 2 در 1 معمولاً در مدل های ساده تر مورد استفاده قرار می گیرد. به عنوان مثال، در رگرسیون خطی با یک ویژگی ورودی و یک بایاس، ماتریس وزن ها ممکن است 2 در 1 باشد، زیرا یک وزن برای ویژگی ورودی و یک وزن برای بایاس داریم.\n",
    "\n",
    "<code style=\"display:block\">#حلقه آموزشی\n",
    "for epoch in range(n_epochs): \n",
    "  m باید به عنوان تعداد نمونه ها (از قبل) در مجموعه داده تعریف شود \n",
    "   for i in range(m):\n",
    "         یک ایندکس تصادفی را انتخاب کنید\n",
    "        random_index = np.random.randint(m)\n",
    "         استخراج features برای نمونه انتخاب شده \n",
    "        xi = X_b[random_index : random_index + 1]         \n",
    "         استخراج target برای نمونه انتخاب شده \n",
    "        yi = y[random_index : random_index + 1]  \n",
    "         محاسبه گرادیان تابع هزینه\n",
    "        gradients = 2 * xi.T.dot(xi.dot(w) - yi) \n",
    "         به روزرسانی پارامترها\n",
    "        w = w - alpha * gradients # اعمال گام نزول گرادیان\n",
    "</code>\n",
    "\n",
    "\n",
    "<b>for epoch in range(n_epochs):</b> یک حلقه برای تعداد دوره ها (n_epochs) ایجاد می کند. در هر دوره، مدل کل مجموعه داده را مرور می کند.\n",
    "\n",
    "<b>for i in range(m):</b> یک حلقه برای تعداد نمونه ها (m) در هر دوره ایجاد می کند. (در اینجا، m تعداد نمونه های داده است که باید در جای دیگری تعریف شده باشد.)\n",
    "\n",
    "<b>random_index = np.random.randint(m):</b> یک شاخص تصادفی بین ۰ و m (تعداد کل نمونه ها) انتخاب می کند. این به مدل کمک می کند تا هر بار یک نمونه تصادفی را برای آموزش انتخاب کند.\n",
    "\n",
    "<b>xi = X_b[random_index : random_index + 1]:</b> ویژگی های نمونه انتخاب شده را استخراج می کند. (فرض کنید X_b ماتریس ویژگی ها است.)\n",
    "\n",
    "<b>yi = y[random_index : random_index + 1]:</b> هدف یا خروجی واقعی نمونه انتخاب شده را استخراج می کند. (فرض کنید y بردار اهداف است.)\n",
    "\n",
    " محاسبه گرادیان و به روزرسانی پارامترها\n",
    "\n",
    "<b>gradients = 2 * xi.T.dot(xi.dot(w) - yi):<b> گرادیان تابع هزینه را محاسبه می کند.\n",
    "\n",
    "<b>xi.dot(w):</b> پیش بینی مدل برای نمونه انتخاب شده را محاسبه می کند.\n",
    "    \n",
    "<b>xi.dot(w) - yi:</b> خطا (یا باقی مانده) بین پیش بینی مدل و هدف واقعی را محاسبه می کند.\n",
    "    \n",
    "<b>xi.T.dot(...):</b> گرادیان را با توجه به وزن ها محاسبه می کند.\n",
    "    \n",
    "<b>2 * ...:</b> ضرب در ۲ برای مقیاسبندی گرادیان (در اینجا تابع هزینه به احتمال زیاد مربع خطا است).\n",
    "    \n",
    "<b>w = w - alpha * gradients:</b> پارامترها را به روزرسانی می کند.\n",
    "    \n",
    "<b>w - alpha * gradients:</b> وزن ها را با کاهش مقدار گرادیان (با ضرب در نرخ یادگیری) به روزرسانی می کند. این عملیات باعث می شود که مدل به سمت کمینه تابع هزینه حرکت کند.\n",
    "    \n",
    " جمع بندی\n",
    "    \n",
    "این کد یک مثال از آموزش یک مدل یادگیری ماشین با استفاده از الگوریتم نزول گرادیان است. به طور کلی، مراحل به شرح زیر هستند:\n",
    "\n",
    "تنظیم تعداد دوره ها و نرخ یادگیری.\n",
    "    \n",
    "مقداردهی اولیه پارامترها به صورت تصادفی.\n",
    "    \n",
    "برای هر دوره و هر نمونه:\n",
    "    \n",
    "انتخاب یک نمونه تصادفی.\n",
    "    \n",
    "محاسبه گرادیان تابع هزینه.\n",
    "    \n",
    "به روزرسانی پارامترها با استفاده از نرخ یادگیری و گرادیان.\n",
    "    \n",
    "امیدوارم که این توضیحات به شما در درک بهتر کد و مفاهیم اساسی یادگیری ماشین کمک کند. اگر سوال دیگری دارید یا نیاز به توضیح بیشتری دارید، حتماً بپرسید!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent\n",
    "\n",
    ":::{dropdown}\n",
    "Dropdown content\n",
    ":::\n",
    "\n",
    ":::{dropdown} Dropdown title\n",
    "Dropdown content\n",
    ":::\n",
    "\n",
    ":::{dropdown} Open dropdown\n",
    ":open:\n",
    "\n",
    "Dropdown content\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 20 # size of mini-batch\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "w = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, batch_size):\n",
    "        xi = X_b_shuffled[i:i+batch_size]\n",
    "        yi = y_shuffled[i:i+batch_size]\n",
    "        gradients = 2/batch_size * xi.T.dot(xi.dot(w) - yi)\n",
    "        w = w - alpha * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
